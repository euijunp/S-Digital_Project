{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e984e3-df51-43b9-a5b8-5faa5f10ca42",
   "metadata": {},
   "source": [
    "## 정상 이메일과 피싱이메일의 데이터를 결합하여 하나의 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfc73a-901a-476c-b1b2-78f72ccee6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 파일 경로 설정\n",
    "parsed_emails_path = \"parsed_emails.json\"  # 정상 이메일 데이터셋\n",
    "generated_emails_path = \"generated_emails.json\"  # 피싱 이메일 데이터셋\n",
    "combined_dataset_path = \"combined_emails_dataset.json\"  # 결합된 데이터셋\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open(parsed_emails_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    normal_emails = json.load(file)\n",
    "\n",
    "with open(generated_emails_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    phishing_emails = json.load(file)\n",
    "\n",
    "# 레이블 추가\n",
    "for email in normal_emails:\n",
    "    email[\"label\"] = 0  # 정상 이메일 레이블: 0\n",
    "\n",
    "for email in phishing_emails:\n",
    "    email[\"label\"] = 1  # 피싱 이메일 레이블: 1\n",
    "\n",
    "# 데이터 결합\n",
    "combined_emails = normal_emails + phishing_emails\n",
    "\n",
    "# 결합된 데이터셋 저장\n",
    "with open(combined_dataset_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(combined_emails, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Combined dataset saved to {combined_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca202a-d860-4eb7-ab20-349e5172844a",
   "metadata": {},
   "source": [
    "## 데이터셋 로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6770333-e5f0-4b74-ac73-77afce78e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "combined_dataset_path = \"combined_emails_dataset.json\"\n",
    "\n",
    "# 결합된 데이터 로드\n",
    "with open(combined_dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    combined_emails = json.load(file)\n",
    "\n",
    "# 텍스트와 레이블 추출\n",
    "texts = [email[\"content\"] for email in combined_emails]\n",
    "labels = [email[\"label\"] for email in combined_emails]\n",
    "\n",
    "# 데이터셋 분리 (80% 학습, 20% 테스트)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830896c0-5d05-4664-bae5-1a138a5aeb05",
   "metadata": {},
   "source": [
    "## NLP 모델 준비 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef098ef-d2b9-4068-a0d7-a08bbecb86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/120 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x14f49c040> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x14f49c040> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function infer_framework at 0x14f49c040> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 60/120 [05:38<05:23,  5.40s/batch]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 텍스트 데이터 토큰화\n",
    "train_encodings = tokenizer(\n",
    "    train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"tf\"\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# TensorFlow 데이터셋 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(16)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# tqdm을 사용하여 학습 진행 상황 표시\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    with tqdm(total=len(train_dataset), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for batch in train_dataset:\n",
    "            model.train_on_batch(batch[0], batch[1])\n",
    "            pbar.update(1)\n",
    "    with tqdm(total=len(test_dataset), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "        for batch in test_dataset:\n",
    "            model.test_on_batch(batch[0], batch[1])\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# 모델 저장 경로\n",
    "save_directory = \"./saved_model\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# 압축 파일 저장 경로\n",
    "compressed_file_path = \"./saved_model.tar.gz\"\n",
    "\n",
    "# tar.gz 형식으로 압축\n",
    "with tarfile.open(compressed_file_path, \"w:gz\") as tar:\n",
    "    tar.add(save_directory, arcname=os.path.basename(save_directory))\n",
    "\n",
    "print(f\"Model and tokenizer saved and compressed at {compressed_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6470e-c8c8-4b32-9462-a8341a6d67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=1).numpy()\n",
    "\n",
    "# 성능 평가\n",
    "print(classification_report(test_labels, predicted_labels, target_names=[\"Legitimate\", \"Phishing\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
