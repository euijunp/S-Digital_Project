{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e984e3-df51-43b9-a5b8-5faa5f10ca42",
   "metadata": {},
   "source": [
    "## 정상 이메일과 피싱이메일의 데이터를 결합하여 하나의 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cfc73a-901a-476c-b1b2-78f72ccee6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to combined_emails_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 파일 경로 설정\n",
    "parsed_emails_path = \"parsed_emails.json\"  # 정상 이메일 데이터셋\n",
    "generated_emails_path = \"generated_emails.json\"  # 피싱 이메일 데이터셋\n",
    "combined_dataset_path = \"combined_emails_dataset.json\"  # 결합된 데이터셋\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open(parsed_emails_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    normal_emails = json.load(file)\n",
    "\n",
    "with open(generated_emails_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    phishing_emails = json.load(file)\n",
    "\n",
    "# 레이블 추가\n",
    "for email in normal_emails:\n",
    "    email[\"label\"] = 0  # 정상 이메일 레이블: 0\n",
    "\n",
    "for email in phishing_emails:\n",
    "    email[\"label\"] = 1  # 피싱 이메일 레이블: 1\n",
    "\n",
    "# 데이터 결합\n",
    "combined_emails = normal_emails + phishing_emails\n",
    "\n",
    "# 결합된 데이터셋 저장\n",
    "with open(combined_dataset_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(combined_emails, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Combined dataset saved to {combined_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca202a-d860-4eb7-ab20-349e5172844a",
   "metadata": {},
   "source": [
    "## 데이터셋 로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6770333-e5f0-4b74-ac73-77afce78e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 결합된 데이터 로드\n",
    "with open(combined_dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    combined_emails = json.load(file)\n",
    "\n",
    "# 텍스트와 레이블 추출\n",
    "texts = [email[\"content\"] for email in combined_emails]\n",
    "labels = [email[\"label\"] for email in combined_emails]\n",
    "\n",
    "# 데이터셋 분리 (80% 학습, 20% 테스트)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830896c0-5d05-4664-bae5-1a138a5aeb05",
   "metadata": {},
   "source": [
    "## NLP 모델 준비 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef098ef-d2b9-4068-a0d7-a08bbecb86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lhywk\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lhywk\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\lhywk\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lhywk\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "52/90 [================>.............] - ETA: 10:13 - loss: 0.1534 - accuracy: 0.9303"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 텍스트 데이터 토큰화\n",
    "train_encodings = tokenizer(\n",
    "    train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"tf\"\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# TensorFlow 데이터셋 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(16)\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(train_dataset, validation_data=test_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6470e-c8c8-4b32-9462-a8341a6d67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=1).numpy()\n",
    "\n",
    "# 성능 평가\n",
    "print(classification_report(test_labels, predicted_labels, target_names=[\"Legitimate\", \"Phishing\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
